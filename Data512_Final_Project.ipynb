{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"i1\"> Data 512 - A6 : Predicting Earthquakes : Final Project</h1>\n",
    "\n",
    "Gautam Moogimane <br>\n",
    "University of Washington - Fall 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"i1\">I. Introduction </h2>     \n",
    "  \n",
    "Having stayed in Japan for a long time, I've been accustomed to getting up at odd hours, with everything shaking around me. Tremors and earthquakes are so common, its rare for a week to go by without one. Considering the amount of damage these natural disasters are capable of, the fact that we are no closer to predicting them than we were a 100 years ago, is slightly surprising to say the least. With this analysis, I aim to use machine learning to analyze historic earthquake data from around the world, and use to it to hopefully come up with some observations about earthquakes in the future.\n",
    "\n",
    "As per the statistics, there are hundreds of earthquakes that happen every day around the globe, with magnitudes ranging from 2-4 on the richter scale. The larger ones, between 5-7 on the scale occur every week or so, while the biggest at 7.5 and above are seen once a year. The 2011 Japan earthquake with its epicenter in Fukushima only had a magnitude of about 6.6, but was strong enough to cause buildings in Tokyo, which is a 100 miles away from Fukushima, to sway for around 30 seconds. Electricty went down, trains were stalled on the tracks, traffic came to a standstill, and thousands of people took to the streets, walking many miles to get back home. I was reading a paper on the economic damages caused by this earthquake, and it was in the range of around 360 billion. This is about 6% of Japans revenue for that year.\n",
    "\n",
    "The aim of this study is not to break some new ground as far as predicting earthquakes go, but a way to see how appropriate or accurate machine learning can be in this situation. Statistics deals with probabilites, and machine learning, which applies these concepts on a computer, can only predict with a certain amount of accuracy, based on the data that it is trained on. To accurately predict earthquakes, one needs to be able to say with a degree of certainty, what the exact location, time of occurance and the magnitude will be. Considering the data we have, and how spread out the locations are, it would be an unrealistic goal trying to achieve this using machine learning at this point in time. Instead, with this analysis, given a certain location(like country with latitudes and longitudes), and time( year, instead of down to the exact second), if we can come up with information about the magnitude of the next earthquake/earthquakes, based on what happened in the past, that would be a good start. \n",
    "\n",
    "From a human centered ethics perspective, the system needs to ensure that any inherent bias in the data does not trickle down in the output generated. From a personal standpoint, I would need to ensure my experience in Japan, does not make me expect or not expect certain results from the model, thereby skewing the predictions in a particular direction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"i2\">II. Background </h2>\n",
    "\n",
    "There has been plenty of work done in this space, and some interesting work that I happened to come across are detailed below <br>\n",
    "[Machine learning predicts earthquakes in a lab](https://www.cam.ac.uk/research/news/machine-learning-used-to-predict-earthquakes-in-a-lab-setting) <br>\n",
    "This is a fascinating read of a team of researchers from University of Cambridge, Los Alamos National Laboratory and Boston University, who were able to identify signals that preempt the arrival of an earthquake under controlled settings. They had steel blocks set up to replicate the physical forces at work under the earths surface, and found particular acoustic signals being generated by the faults, just before an earthquake. Using a machine learning algorithm, that was fed this signal, it was able to identify instances when the fault was under stress, and about to cause an earthquake. <br>\n",
    "An obvious drawback, is that the actual forces under the earths surface, are a lot different than what is found under controlled settings. So the sucess they have had in a lab, has not yet been replicated in actual settings. But the results are encouraging. <br>\n",
    "\n",
    "Another paper that is an interesting read, is the research done in the Hindukush region, to predict the magnitude of an earthquake using machine learning. <br>\n",
    "[Hindukush earthquake magnitude prediction](https://www.researchgate.net/publication/307951466_Earthquake_magnitude_prediction_in_Hindukush_region_using_machine_learning_techniques) <br>\n",
    "They use mathematically calculated eight seismic indicators, and then apply four machine learning algorithms,\n",
    "pattern recognition neural network, recurrent neural network, random forest and linear programming boost ensemble classifier to model relationships between the indicators and future earthquake occurences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"i3\">III. Data</h2> \n",
    "\n",
    "The data is from the significant earthquake database, and contains historic data from 2150 BC to the present day, of earthquakes from all over the world. The data is constantly updated to reflect the latest events.\n",
    "\n",
    "\n",
    "As per the description from the 'data.nodc.noaa.gov' site where the data is hosted:\n",
    " > The Significant Earthquake Database is a global listing of over 5,700 earthquakes from 2150 BC to the present. A significant earthquake is classified as one that meets at least one of the following criteria:\n",
    " \n",
    " > caused deaths, caused moderate damage (approximately 1 million dollars or more), magnitude 7.5 or greater, Modified Mercalli Intensity (MMI) X or greater, or the earthquake generated a tsunami. \n",
    " \n",
    " > The database provides information on the date and time of occurrence, latitude and longitude, focal depth, magnitude, maximum MMI intensity, and socio-economic data such as the total number of casualties, injuries, houses destroyed, and houses damaged, and dollar damage estimates. \n",
    "References, political geography, and additional comments are also provided for each earthquake. If the earthquake was associated with a tsunami or volcanic eruption, it is flagged and linked to the related tsunami event or significant volcanic eruption.\n",
    " \n",
    "\n",
    "The fields that are key from the perspective of our analysis are detailed in the table below <br>\n",
    "\n",
    "| Field Name    | Datatype      | Description |\n",
    "| :-------------: | :-------------: | :-------------: |\n",
    "|  Year | Integer | The year of occurence  |\n",
    "| Focal Depth  | Integer  | Depth of the epicenter |\n",
    "| Eq_Primary | Float | Magnitude of the earthquake |\n",
    "| Country | String | Name of the country |\n",
    "| Location_Name | String | Specific location in the country | \n",
    "| Latitude | Float | Coordinates of the exact location |\n",
    "| Longitude | Float | Coordinates of the exact location |\n",
    "\n",
    "\n",
    "\n",
    "The data can be downloaded in a tab separated file, and can be imported into excel or python for analysis. <br>\n",
    "We start by ingesting the data from the link provided below <br>\n",
    "[National Geophysical Data Center / World Data Service (NGDC/WDS): Significant Earthquake Database. National Geophysical Data Center, NOAA](http://dx.doi.org/10.7289/V5TD9V7K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-1 Ingesting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# the data is contained in a tab seperated file\n",
    "raw_data = pd.read_csv('earthquake_historical.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I_D</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>HOUR</th>\n",
       "      <th>MINUTE</th>\n",
       "      <th>FOCAL_DEPTH</th>\n",
       "      <th>EQ_PRIMARY</th>\n",
       "      <th>EQ_MAG_MW</th>\n",
       "      <th>EQ_MAG_MS</th>\n",
       "      <th>...</th>\n",
       "      <th>TOTAL_MISSING</th>\n",
       "      <th>TOTAL_MISSING_DESCRIPTION</th>\n",
       "      <th>TOTAL_INJURIES</th>\n",
       "      <th>TOTAL_INJURIES_DESCRIPTION</th>\n",
       "      <th>TOTAL_DAMAGE_MILLIONS_DOLLARS</th>\n",
       "      <th>TOTAL_DAMAGE_DESCRIPTION</th>\n",
       "      <th>TOTAL_HOUSES_DESTROYED</th>\n",
       "      <th>TOTAL_HOUSES_DESTROYED_DESCRIPTION</th>\n",
       "      <th>TOTAL_HOUSES_DAMAGED</th>\n",
       "      <th>TOTAL_HOUSES_DAMAGED_DESCRIPTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6067.000000</td>\n",
       "      <td>6067.000000</td>\n",
       "      <td>5662.000000</td>\n",
       "      <td>5510.000000</td>\n",
       "      <td>4039.000000</td>\n",
       "      <td>3833.00000</td>\n",
       "      <td>3115.000000</td>\n",
       "      <td>4280.000000</td>\n",
       "      <td>1212.000000</td>\n",
       "      <td>2916.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1180.000000</td>\n",
       "      <td>1354.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>3143.000000</td>\n",
       "      <td>7.780000e+02</td>\n",
       "      <td>1705.000000</td>\n",
       "      <td>3.700000e+02</td>\n",
       "      <td>709.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4653.714192</td>\n",
       "      <td>1802.129718</td>\n",
       "      <td>6.506711</td>\n",
       "      <td>15.730127</td>\n",
       "      <td>11.304283</td>\n",
       "      <td>28.85964</td>\n",
       "      <td>41.725201</td>\n",
       "      <td>6.472220</td>\n",
       "      <td>6.527805</td>\n",
       "      <td>6.574451</td>\n",
       "      <td>...</td>\n",
       "      <td>2016.958333</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2518.709322</td>\n",
       "      <td>1.972674</td>\n",
       "      <td>1978.743206</td>\n",
       "      <td>2.194082</td>\n",
       "      <td>1.892610e+04</td>\n",
       "      <td>2.709091</td>\n",
       "      <td>6.688065e+04</td>\n",
       "      <td>2.421721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2921.242903</td>\n",
       "      <td>378.029895</td>\n",
       "      <td>3.448853</td>\n",
       "      <td>8.751046</td>\n",
       "      <td>7.033406</td>\n",
       "      <td>17.15401</td>\n",
       "      <td>71.307850</td>\n",
       "      <td>1.043618</td>\n",
       "      <td>0.936106</td>\n",
       "      <td>0.989850</td>\n",
       "      <td>...</td>\n",
       "      <td>8841.694676</td>\n",
       "      <td>1.154701</td>\n",
       "      <td>28320.900017</td>\n",
       "      <td>1.082683</td>\n",
       "      <td>12988.187606</td>\n",
       "      <td>1.041105</td>\n",
       "      <td>1.998497e+05</td>\n",
       "      <td>1.048056</td>\n",
       "      <td>1.091931e+06</td>\n",
       "      <td>1.124273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2150.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2140.500000</td>\n",
       "      <td>1817.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>14.00000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>5.700000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.310000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.625000e+01</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.375000e+01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4606.000000</td>\n",
       "      <td>1927.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.370000e+02</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.145000e+02</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6471.500000</td>\n",
       "      <td>1986.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>44.00000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>7.300000</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>7.300000</td>\n",
       "      <td>...</td>\n",
       "      <td>153.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.964500e+03</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.790000e+03</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10373.000000</td>\n",
       "      <td>2018.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>59.00000</td>\n",
       "      <td>675.000000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>43476.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>799000.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>220085.456000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.360000e+06</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.100000e+07</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                I_D         YEAR        MONTH          DAY         HOUR  \\\n",
       "count   6067.000000  6067.000000  5662.000000  5510.000000  4039.000000   \n",
       "mean    4653.714192  1802.129718     6.506711    15.730127    11.304283   \n",
       "std     2921.242903   378.029895     3.448853     8.751046     7.033406   \n",
       "min        1.000000 -2150.000000     1.000000     1.000000     0.000000   \n",
       "25%     2140.500000  1817.500000     4.000000     8.000000     5.000000   \n",
       "50%     4606.000000  1927.000000     7.000000    16.000000    11.000000   \n",
       "75%     6471.500000  1986.000000     9.000000    23.000000    17.000000   \n",
       "max    10373.000000  2018.000000    12.000000    31.000000    23.000000   \n",
       "\n",
       "           MINUTE  FOCAL_DEPTH   EQ_PRIMARY    EQ_MAG_MW    EQ_MAG_MS  ...  \\\n",
       "count  3833.00000  3115.000000  4280.000000  1212.000000  2916.000000  ...   \n",
       "mean     28.85964    41.725201     6.472220     6.527805     6.574451  ...   \n",
       "std      17.15401    71.307850     1.043618     0.936106     0.989850  ...   \n",
       "min       0.00000     0.000000     1.600000     3.600000     2.100000  ...   \n",
       "25%      14.00000    11.000000     5.700000     5.800000     5.800000  ...   \n",
       "50%      30.00000    26.000000     6.500000     6.500000     6.600000  ...   \n",
       "75%      44.00000    40.000000     7.300000     7.200000     7.300000  ...   \n",
       "max      59.00000   675.000000     9.500000     9.500000     9.100000  ...   \n",
       "\n",
       "       TOTAL_MISSING  TOTAL_MISSING_DESCRIPTION  TOTAL_INJURIES  \\\n",
       "count      24.000000                  22.000000     1180.000000   \n",
       "mean     2016.958333                   2.000000     2518.709322   \n",
       "std      8841.694676                   1.154701    28320.900017   \n",
       "min         1.000000                   1.000000        1.000000   \n",
       "25%         5.750000                   1.000000       10.000000   \n",
       "50%        25.500000                   1.500000       42.500000   \n",
       "75%       153.500000                   3.000000      200.000000   \n",
       "max     43476.000000                   4.000000   799000.000000   \n",
       "\n",
       "       TOTAL_INJURIES_DESCRIPTION  TOTAL_DAMAGE_MILLIONS_DOLLARS  \\\n",
       "count                 1354.000000                     418.000000   \n",
       "mean                     1.972674                    1978.743206   \n",
       "std                      1.082683                   12988.187606   \n",
       "min                      1.000000                       0.010000   \n",
       "25%                      1.000000                       4.310000   \n",
       "50%                      2.000000                      28.000000   \n",
       "75%                      3.000000                     300.000000   \n",
       "max                      4.000000                  220085.456000   \n",
       "\n",
       "       TOTAL_DAMAGE_DESCRIPTION  TOTAL_HOUSES_DESTROYED  \\\n",
       "count               3143.000000            7.780000e+02   \n",
       "mean                   2.194082            1.892610e+04   \n",
       "std                    1.041105            1.998497e+05   \n",
       "min                    1.000000            1.000000e+00   \n",
       "25%                    1.000000            6.625000e+01   \n",
       "50%                    2.000000            5.370000e+02   \n",
       "75%                    3.000000            3.964500e+03   \n",
       "max                    4.000000            5.360000e+06   \n",
       "\n",
       "       TOTAL_HOUSES_DESTROYED_DESCRIPTION  TOTAL_HOUSES_DAMAGED  \\\n",
       "count                         1705.000000          3.700000e+02   \n",
       "mean                             2.709091          6.688065e+04   \n",
       "std                              1.048056          1.091931e+06   \n",
       "min                              1.000000          1.000000e+00   \n",
       "25%                              2.000000          8.375000e+01   \n",
       "50%                              3.000000          6.145000e+02   \n",
       "75%                              4.000000          2.790000e+03   \n",
       "max                              4.000000          2.100000e+07   \n",
       "\n",
       "       TOTAL_HOUSES_DAMAGED_DESCRIPTION  \n",
       "count                        709.000000  \n",
       "mean                           2.421721  \n",
       "std                            1.124273  \n",
       "min                            1.000000  \n",
       "25%                            1.000000  \n",
       "50%                            2.000000  \n",
       "75%                            3.000000  \n",
       "max                            4.000000  \n",
       "\n",
       "[8 rows x 40 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe the data\n",
    "raw_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-2 Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing to check is what the EQ fields other than EQ_PRIMARY contain, and if at any point they contain different values. <br>\n",
    "We intend to use EQ_PRIMARY, but need to make sure the other fields do not contain additional information.<br>\n",
    "Also dropping rows where EQ_PRIMARY or EQ_MAG_MS is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7592592592592593"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop empty rows\n",
    "comp = raw_data.dropna(subset=['EQ_PRIMARY', 'EQ_MAG_MS'])\n",
    "comp = comp[['I_D','EQ_PRIMARY','EQ_MAG_MS']]\n",
    "# compare the values of EQ_PRIMARY and EQ_MAG_MS\n",
    "comp['Result'] = np.where(comp['EQ_PRIMARY'] == comp['EQ_MAG_MS'], 'TRUE', 'FALSE')\n",
    "# find the ratio of rows that match to total rows \n",
    "(len(comp[comp.Result == 'TRUE']))/(len(comp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 25% of the values seem to be different for the 2 columns. I suspect the different EQ columns \n",
    "are just recording values from different sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another point to check is if there are rows where EQ_PRIMARY is null and data exists in other 6 EQ rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I_D</th>\n",
       "      <th>FLAG_TSUNAMI</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>HOUR</th>\n",
       "      <th>MINUTE</th>\n",
       "      <th>SECOND</th>\n",
       "      <th>FOCAL_DEPTH</th>\n",
       "      <th>EQ_PRIMARY</th>\n",
       "      <th>...</th>\n",
       "      <th>TOTAL_MISSING</th>\n",
       "      <th>TOTAL_MISSING_DESCRIPTION</th>\n",
       "      <th>TOTAL_INJURIES</th>\n",
       "      <th>TOTAL_INJURIES_DESCRIPTION</th>\n",
       "      <th>TOTAL_DAMAGE_MILLIONS_DOLLARS</th>\n",
       "      <th>TOTAL_DAMAGE_DESCRIPTION</th>\n",
       "      <th>TOTAL_HOUSES_DESTROYED</th>\n",
       "      <th>TOTAL_HOUSES_DESTROYED_DESCRIPTION</th>\n",
       "      <th>TOTAL_HOUSES_DAMAGED</th>\n",
       "      <th>TOTAL_HOUSES_DAMAGED_DESCRIPTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [I_D, FLAG_TSUNAMI, YEAR, MONTH, DAY, HOUR, MINUTE, SECOND, FOCAL_DEPTH, EQ_PRIMARY, EQ_MAG_MW, EQ_MAG_MS, EQ_MAG_MB, EQ_MAG_ML, EQ_MAG_MFA, EQ_MAG_UNK, INTENSITY, COUNTRY, STATE, LOCATION_NAME, LATITUDE, LONGITUDE, REGION_CODE, DEATHS, DEATHS_DESCRIPTION, MISSING, MISSING_DESCRIPTION, INJURIES, INJURIES_DESCRIPTION, DAMAGE_MILLIONS_DOLLARS, DAMAGE_DESCRIPTION, HOUSES_DESTROYED, HOUSES_DESTROYED_DESCRIPTION, HOUSES_DAMAGED, HOUSES_DAMAGED_DESCRIPTION, TOTAL_DEATHS, TOTAL_DEATHS_DESCRIPTION, TOTAL_MISSING, TOTAL_MISSING_DESCRIPTION, TOTAL_INJURIES, TOTAL_INJURIES_DESCRIPTION, TOTAL_DAMAGE_MILLIONS_DOLLARS, TOTAL_DAMAGE_DESCRIPTION, TOTAL_HOUSES_DESTROYED, TOTAL_HOUSES_DESTROYED_DESCRIPTION, TOTAL_HOUSES_DAMAGED, TOTAL_HOUSES_DAMAGED_DESCRIPTION]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 47 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter the data to rows where EQ_PRIMARY is null\n",
    "temp = raw_data[pd.isnull(raw_data['EQ_PRIMARY'])]\n",
    "# check for cases where the other rows are not null\n",
    "temp[(temp.EQ_MAG_MS.notnull()) | (temp.EQ_MAG_MW.notnull()) | (temp.EQ_MAG_MB.notnull()) | (temp.EQ_MAG_ML.notnull()) | (temp.EQ_MAG_MFA.notnull()) | (temp.EQ_MAG_UNK.notnull())]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out there are no such cases.<br>\n",
    "We will continue to stick with EQ_PRIMARY for our analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fields we are mainly concerned with are 'YEAR','MONTH','DAY','EQ_PRIMARY','COUNTRY','LATITUDE' and 'LONGITUDE'.\n",
    "Creating a new dataframe with these fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>MAGNITUDE</th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.3</td>\n",
       "      <td>JORDAN</td>\n",
       "      <td>31.100</td>\n",
       "      <td>35.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.1</td>\n",
       "      <td>TURKMENISTAN</td>\n",
       "      <td>38.000</td>\n",
       "      <td>58.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.5</td>\n",
       "      <td>ISRAEL</td>\n",
       "      <td>32.000</td>\n",
       "      <td>35.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.2</td>\n",
       "      <td>JORDAN</td>\n",
       "      <td>29.600</td>\n",
       "      <td>35.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-479</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>GREECE</td>\n",
       "      <td>39.700</td>\n",
       "      <td>23.300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    YEAR  MONTH  DAY  MAGNITUDE       COUNTRY  LATITUDE  LONGITUDE\n",
       "0  -2150    NaN  NaN        7.3        JORDAN    31.100     35.500\n",
       "2  -2000    NaN  NaN        7.1  TURKMENISTAN    38.000     58.200\n",
       "7  -1250    NaN  NaN        6.5        ISRAEL    32.000     35.500\n",
       "8  -1050    NaN  NaN        6.2        JORDAN    29.600     35.000\n",
       "14  -479    NaN  NaN        7.0        GREECE    39.700     23.300"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eq_df = raw_data[['YEAR','MONTH','DAY','EQ_PRIMARY','COUNTRY','LATITUDE','LONGITUDE']]\n",
    "# renaming EQ_PRIMARY to MAGNITUDE\n",
    "eq_df = eq_df.rename(columns={\"EQ_PRIMARY\":\"MAGNITUDE\"})\n",
    "# rows where the magnitude is missing, can be deleted as they are of no use to us.\n",
    "eq_df = eq_df[eq_df.MAGNITUDE.notnull()]\n",
    "eq_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"i4\">IV. Methods</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Basemap python package along with matplotlib, we can plot the different latitudes and longitudes on a world map, and color code the different points based on the magnitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for projection on a world map\n",
    "# extract latitudes,longitudes and magnitude from the dataframe as a list\n",
    "eq_df['LATITUDE'] = lat = pd.to_numeric(eq_df['LATITUDE'], errors='coerce').tolist() \n",
    "eq_df['LONGITUDE'] = lon = pd.to_numeric(eq_df['LONGITUDE'], errors='coerce').tolist()\n",
    "eq_df['MAGNITUDE'] = mag = pd.to_numeric(eq_df['MAGNITUDE'], errors='coerce').tolist()\n",
    "# some rows have null data, these can be manually filled in\n",
    "eq_df.loc[1759,'LATITUDE'] = 40.9006\n",
    "eq_df.loc[1759,'LONGITUDE'] = 174.8860\n",
    "eq_df.loc[2261,'LATITUDE'] = 0.7893\n",
    "eq_df.loc[3256,'LATITUDE'] = 0.7893\n",
    "eq_df.loc[3341,'LATITUDE'] = 0.8893\n",
    "eq_df.loc[3382,'LATITUDE'] = 0.8693\n",
    "eq_df.loc[3437,'LATITUDE'] = 1.3733\n",
    "# save the cleaned data\n",
    "eq_df.to_csv('clean_earthquake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'dedent' from 'matplotlib.cbook' (C:\\ProgramData\\Anaconda2021\\lib\\site-packages\\matplotlib\\cbook\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-30c271c6aaa8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasemap\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBasemap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpylab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcsetup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2021\\lib\\site-packages\\mpl_toolkits\\basemap\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimread\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mproj\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mProj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2021\\lib\\site-packages\\mpl_toolkits\\basemap\\proj.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyproj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcbook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdedent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0m__version__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'1.2.2'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'dedent' from 'matplotlib.cbook' (C:\\ProgramData\\Anaconda2021\\lib\\site-packages\\matplotlib\\cbook\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from . import cbook, rcsetup\n",
    "from matplotlib.cbook import MatplotlibDeprecationWarning, sanitize_sequence\n",
    "from matplotlib.cbook import mplDeprecation  # deprecated\n",
    "from matplotlib.rcsetup import validate_backend, cycler\n",
    "\n",
    "# set the size of the plot in width and height\n",
    "rcParams['figure.figsize'] = (16,12)\n",
    "# lon_0 is central longitude of projection.\n",
    "# resolution = 'l' means use low resolution, which loads faster and works for a large map.\n",
    "eq_map = Basemap(projection='robin', lon_0=0, lat_0=0, resolution='l', area_thresh=1500.0)\n",
    "eq_map.drawcoastlines()\n",
    "eq_map.fillcontinents(color='gray')\n",
    "eq_map.drawmapboundary(fill_color='white')\n",
    "x,y = eq_map(lon, lat)\n",
    "# ro stands for red color and 'o' marker\n",
    "eq_map.plot(x, y, 'ro', markersize=4)\n",
    "plt.title(\"Map of all Earthquakes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rcParams' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-0755849f6e85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m                           markersize=5, label='Magnitude greater than 6')\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'figure.figsize'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;31m# lon_0 is central longitude of projection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# resolution = 'l' means use low resolution, which loads faster and works for a large map.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rcParams' is not defined"
     ]
    }
   ],
   "source": [
    "# for a slightly different perspective, lets visualize different colors based on the magnitude \n",
    "import matplotlib.lines as mlines\n",
    "def color(magnitude):\n",
    "    # Returns green for small earthquakes, yellow for medium earthquakes, and red for large earthquakes.\n",
    "    if magnitude < 4.0:\n",
    "        return ('go')\n",
    "    elif magnitude < 6.0:\n",
    "        return ('yo')\n",
    "    else:\n",
    "        return ('ro')\n",
    "# specify the labels for the legend \n",
    "green_line = mlines.Line2D([], [], color='green', marker='o',\n",
    "                          markersize=5, label='Magnitude less than 4')\n",
    "yellow_line = mlines.Line2D([], [], color='yellow', marker='o',\n",
    "                          markersize=5, label='Magnitude between 4 and 6')\n",
    "red_line = mlines.Line2D([], [], color='red', marker='o',\n",
    "                          markersize=5, label='Magnitude greater than 6')\n",
    "\n",
    "rcParams['figure.figsize'] = (16,12)\n",
    "# lon_0 is central longitude of projection.\n",
    "# resolution = 'l' means use low resolution, which loads faster and works for a large map.\n",
    "eq_map_col = Basemap(projection='robin', lon_0=0, lat_0=0, resolution='l', area_thresh=1500.0)\n",
    "eq_map_col.drawcoastlines()\n",
    "eq_map_col.fillcontinents(color='gray')\n",
    "eq_map_col.drawmapboundary(fill_color='white')\n",
    "# go through each value and plot with color according to the magnitude\n",
    "for lo, la, ma in zip(lon, lat, mag):\n",
    "    x,y = eq_map_col(lo, la)\n",
    "    col = color(ma)\n",
    "    eq_map_col.plot(x, y, col, markersize=4)\n",
    "\n",
    "plt.title(\"Map of all Earthquakes\")\n",
    "plt.legend(handles=[green_line,yellow_line,red_line],bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running algorithms on the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by trying to fit a linear regression model on the data, which will use a linear combination of the input variables that can best map to the target value . In this instance, I do not think this would be a great fit, as intuitively, it does not seem that there is a linear relationship between Year, latitude and longitude with the magnitude of an earthquake. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for running machine learning algorthms, lets concentrate on the features YEAR,LATITUDE and LONGITUDE with MAGNITUDE as the label\n",
    "# creating a new dataset with these 3 columns\n",
    "eqml_df = eq_df[['YEAR','LATITUDE','LONGITUDE','MAGNITUDE']]\n",
    "X = eq_df[['YEAR','LATITUDE','LONGITUDE']]\n",
    "y = eq_df['MAGNITUDE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into test and train sets using sklearn modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=28)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# initialize a linear regression model\n",
    "regression_model = LinearRegression()\n",
    "# fit the model using the training data above\n",
    "regression_model.fit(X_train, y_train)\n",
    "# after fitting, check the values predicted by the model on the test data\n",
    "regression_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction score for this model on the test data\n",
    "regression_model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As sort of expected, the score is very low on the predicted values, which does not seem to map to the magnitude values from our target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next algorithm is a decision tree regressor, and the way this works is similar to a binary tree with a root and nodes on it. It proceeds down a sequential root answering 'If this then that' questions along the way, that ultimately leads to a result. It works well on both categorical and numerical data, and performance wise is also fast. Some of its downfalls include overfitting, especially if there are many features in the model and the tree runs deep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "mod_tree = tree.DecisionTreeRegressor()\n",
    "mod_tree.fit(X_train,y_train)\n",
    "mod_tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction score for this model on the test data\n",
    "mod_tree.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly the score isnt great here either, on default settings, with max_depth as none and min_samples_split=2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest is an aggregation of multiple decision trees that combines all their results into one final result. This makes it a lot more robust than a single decision tree, and can help reduce errors due to bias and variance, that can creep into decision tree models. It randomly selects a subset of the features for each of its decision trees, which ultimately when combined, tends to cover all the features. Its also fairly resistant to overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# initialize the model\n",
    "reg = RandomForestRegressor(random_state=28)\n",
    "# fit the parameters\n",
    "reg.fit(X_train, y_train)\n",
    "reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This score is a significant improvement over the previous algorithms used, and we can try to further tune it by using cross validation to optimize the values of the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# initialize the parameters in multiples of 10\n",
    "parameters = {'n_estimators':[1,10,100,500,1000]}\n",
    "# initialize the gridsearch cross validation object\n",
    "grid_obj = GridSearchCV(reg, parameters)\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "# find the best fit from the parameters\n",
    "best_fit = grid_fit.best_estimator_\n",
    "best_fit.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model gives us a pretty decent score of around 45%, which we will use for our findings ahead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"i4\">V. Findings </h2> \n",
    "\n",
    "In the following section, we try to answer some of the research questions we had before the analysis began"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What would the approximate magnitude of an earthquake in the US in 2019 be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting using the best fit model, by passing the year as 2019, and the country latitudes and longitudes for the US.\n",
    "x1 = [[2019,37.09,95.71]]\n",
    "best_fit.predict(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can expect an earthquake of magnitude around 5.95 in the US next year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In what year can we expect a large earthquake in the US (magnitude > 7.0)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer this question, we would need to train the model using Year as the output while the magnitude, and country coordinates act as the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model with year as the output label\n",
    "X = eq_df[['MAGNITUDE','LATITUDE','LONGITUDE']]\n",
    "y = eq_df['YEAR']\n",
    "# training and fitting the same model as previously\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=28)\n",
    "reg = RandomForestRegressor(random_state=30)\n",
    "reg.fit(X_train, y_train)\n",
    "grid_obj = GridSearchCV(reg, parameters)\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "best_fit = grid_fit.best_estimator_\n",
    "best_fit.score(X_test,y_test)\n",
    "# the score is a lot less than with Magnitude as the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = [[7.0,37.09,95.71]]\n",
    "best_fit.predict(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with this prediction, is that the Year column is being fed in as integer instead of datetime. Trying to convert this column to datetime, gives an error since python does not seem to work well with negative dates(BC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which country has historically had most number of earthquakes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_df.groupby('COUNTRY').size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping the dataset by Country, looking at the count and ordering by a descending number of the count, gives us the list above. \n",
    "\n",
    "China leads the pack with 559 occurences in the list, which was slightly surprising, but I suspect it has something to do with how the territories were divided between China and Japan historically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2 id=\"i5\">VI. Conclusion </h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Overall, I think this was a learning experience for me personally, where I could experiment with different machine learning algorithms and see how well they scaled with historical earthquake data. <br>\n",
    "What makes predicting earthquakes so challenging, in spite of having detailed historical data, is that it doesn't seem to follow any particular pattern precisely. Fault lines around the world do tend to slip in a somewhat cyclical pattern, but any predictions made using this, is likely to be a window of +-20 years at best, rather than a precise time. <br> This case is best highlighted when in 1981, scientists made a prediction saying the California fault line could trigger an earthquake in the next few months, and in order to monitor the forces before and after the event, huge stations and monitoring equipment was set up all over the place, with a significant investment. It turned out that the earthquake actually occured in 2001, 20 years after they were expecting it. <br>\n",
    "However, machine learning does open up a new perspective to tackle this challenge, and I believe that if we can finetune our algorithms to achieve a high degree of accuracy with the data, it can definitely lead to more accurate predictions, that might not pinpoint the occurrence down to the exact second, but can help reduce the uncertainty window down significantly. That would be a huge step forward. <br>\n",
    "From a human design perspective, the solution must present end users, with enough time to act upon their warnings. There is a system currently in Japan, that can sound an alarm before an earthquake is about to occur. This is apparently the time it takes for the waves to travel from the epicenter to the location of the alarm, which is usually less than 30 seconds. While better than nothing, the window is still too small for people to really act on it, and just provides enough time to duck underneath a sturdy table perhaps. <br>\n",
    "The most promising research in my opinion, comes from trying to mimic the actual forces at play under the ground, in a lab setting, to produce acoustic signals that can then be used to train a machine learning algorithm. The signals produced just before the fault line releases that energy, has a particular sound signature to it, that can be used to identify and predict future occurrences. This method was highlighted in one of the papers referenced above, in the background section. <br>\n",
    "[Machine learning predicts earthquakes in a lab](https://www.cam.ac.uk/research/news/machine-learning-used-to-predict-earthquakes-in-a-lab-setting) <br>\n",
    "There is a lot of exciting ongoing research on this topic, and I do hope one of these days we manage to make a breakthrough, and end one of the long standing mysteries of our planet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2 id=\"i7\">VII. References </h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Machine learning predicts earthquakes in a lab](https://www.cam.ac.uk/research/news/machine-learning-used-to-predict-earthquakes-in-a-lab-setting) <br>\n",
    "[Hindukush earthquake magnitude prediction](https://www.researchgate.net/publication/307951466_Earthquake_magnitude_prediction_in_Hindukush_region_using_machine_learning_techniques) <br>\n",
    "[Analysis of soil radon data using decision trees](https://www.sciencedirect.com/science/article/pii/S0969804303000940)<br>\n",
    "[Using Neural Networks to predict earthquake magnitude](https://www.sciencedirect.com/science/article/pii/S0893608009000926)<br>\n",
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6033417/ <br>\n",
    "https://earthquake.usgs.gov/data/data.php#eq <br>\n",
    "https://arxiv.org/pdf/1702.05774.pdf <br>\n",
    "https://www.scientificamerican.com/article/can-artificial-intelligence-predict-earthquakes/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
